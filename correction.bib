@article{bryantGrammaticalErrorCorrection2023,
  title = {Grammatical {{Error Correction}}: {{A Survey}} of the {{State}} of the {{Art}}},
  shorttitle = {Grammatical {{Error Correction}}},
  author = {Bryant, Christopher and Yuan, Zheng and Qorib, Muhammad Reza and Cao, Hannan and Ng, Hwee Tou and Briscoe, Ted},
  year = {2023},
  month = sep,
  journal = {Computational Linguistics},
  pages = {643--701},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  doi = {10.1162/coli_a_00478},
  urldate = {2025-09-22},
  abstract = {Grammatical Error Correction (GEC) is the task of automatically detecting and correcting errors in text. The task not only includes the correction of grammatical errors, such as missing prepositions and mismatched subject--verb agreement, but also orthographic and semantic errors, such as misspellings and word choice errors, respectively. The field has seen significant progress in the last decade, motivated in part by a series of five shared tasks, which drove the development of rule-based methods, statistical classifiers, statistical machine translation, and finally neural machine translation systems, which represent the current dominant state of the art. In this survey paper, we condense the field into a single article and first outline some of the linguistic challenges of the task, introduce the most popular datasets that are available to researchers (for both English and other languages), and summarize the various methods and techniques that have been developed with a particular focus on artificial error generation. We next describe the many different approaches to evaluation as well as concerns surrounding metric reliability, especially in relation to subjective human judgments, before concluding with an overview of recent progress and suggestions for future work and remaining challenges. We hope that this survey will serve as a comprehensive resource for researchers who are new to the field or who want to be kept apprised of recent developments.},
  file = {/home/schn/snap/zotero-snap/common/Zotero/storage/D3JFB27Q/Bryant et al. - 2023 - Grammatical Error Correction A Survey of the State of the Art.pdf}
}

@misc{ciesielskiNeuralMachineTranslation2024,
  title = {Neural {{Machine Translation Versus Large Language Models}}},
  author = {Ciesielski, Jourik},
  year = {2024},
  month = jun,
  urldate = {2025-09-22},
  abstract = {In this article, leaders from six of the world's most influential language companies share their perspectives on the best approach to automated translation.},
  howpublished = {https://multilingual.com/magazine/june-2024/neural-machine-translation-versus-large-language-models/},
  langid = {american},
  file = {/home/schn/snap/zotero-snap/common/Zotero/storage/92GDH3WQ/neural-machine-translation-versus-large-language-models.html;/home/schn/snap/zotero-snap/common/Zotero/storage/VNJMA5E7/neural-machine-translation-versus-large-language-models.html}
}

@article{homolakExploringAdoptionChatGPT2023,
  title = {Exploring the Adoption of {{ChatGPT}} in Academic Publishing: Insights and Lessons for Scientific Writing},
  shorttitle = {Exploring the Adoption of {{ChatGPT}} in Academic Publishing},
  author = {Homolak, Jan},
  year = {2023},
  month = jun,
  journal = {Croatian Medical Journal},
  volume = {64},
  number = {3},
  pages = {205--207},
  issn = {1332-8166},
  doi = {10.3325/cmj.2023.64.205},
  langid = {english},
  pmcid = {PMC10332292},
  pmid = {37391919}
}

@misc{kobayashiLargeLanguageModels2024,
  title = {Large {{Language Models Are State-of-the-Art Evaluator}} for {{Grammatical Error Correction}}},
  author = {Kobayashi, Masamune and Mita, Masato and Komachi, Mamoru},
  year = {2024},
  month = may,
  number = {arXiv:2403.17540},
  eprint = {2403.17540},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.17540},
  urldate = {2025-09-22},
  abstract = {Large Language Models (LLMs) have been reported to outperform existing automatic evaluation metrics in some tasks, such as text summarization and machine translation. However, there has been a lack of research on LLMs as evaluators in grammatical error correction (GEC). In this study, we investigate the performance of LLMs in GEC evaluation by employing prompts designed to incorporate various evaluation criteria inspired by previous research. Our extensive experimental results demonstrate that GPT-4 achieved Kendall's rank correlation of 0.662 with human judgments, surpassing all existing methods. Furthermore, in recent GEC evaluations, we have underscored the significance of the LLMs scale and particularly emphasized the importance of fluency among evaluation criteria.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/schn/snap/zotero-snap/common/Zotero/storage/YU4UAN7K/Kobayashi et al. - 2024 - Large Language Models Are State-of-the-Art Evaluator for Grammatical Error Correction.pdf;/home/schn/snap/zotero-snap/common/Zotero/storage/I6M6Z9SG/2403.html;/home/schn/snap/zotero-snap/common/Zotero/storage/P5DGYT56/2403.html}
}

@misc{maityHowReadyAre2024,
  title = {How {{Ready Are Generative Pre-trained Large Language Models}} for {{Explaining Bengali Grammatical Errors}}?},
  author = {Maity, Subhankar and Deroy, Aniket and Sarkar, Sudeshna},
  year = {2024},
  month = may,
  number = {arXiv:2406.00039},
  eprint = {2406.00039},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.00039},
  urldate = {2025-09-22},
  abstract = {Grammatical error correction (GEC) tools, powered by advanced generative artificial intelligence (AI), competently correct linguistic inaccuracies in user input. However, they often fall short in providing essential natural language explanations, which are crucial for learning languages and gaining a deeper understanding of the grammatical rules. There is limited exploration of these tools in low-resource languages such as Bengali. In such languages, grammatical error explanation (GEE) systems should not only correct sentences but also provide explanations for errors. This comprehensive approach can help language learners in their quest for proficiency. Our work introduces a real-world, multi-domain dataset sourced from Bengali speakers of varying proficiency levels and linguistic complexities. This dataset serves as an evaluation benchmark for GEE systems, allowing them to use context information to generate meaningful explanations and high-quality corrections. Various generative pre-trained large language models (LLMs), including GPT-4 Turbo, GPT-3.5 Turbo, Text-davinci-003, Text-babbage-001, Text-curie-001, Text-ada-001, Llama-2-7b, Llama-2-13b, and Llama-2-70b, are assessed against human experts for performance comparison. Our research underscores the limitations in the automatic deployment of current state-of-the-art generative pre-trained LLMs for Bengali GEE. Advocating for human intervention, our findings propose incorporating manual checks to address grammatical errors and improve feedback quality. This approach presents a more suitable strategy to refine the GEC tools in Bengali, emphasizing the educational aspect of language learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/schn/snap/zotero-snap/common/Zotero/storage/MUTWZNC7/Maity et al. - 2024 - How Ready Are Generative Pre-trained Large Language Models for Explaining Bengali Grammatical Errors.pdf;/home/schn/snap/zotero-snap/common/Zotero/storage/9KB9NHNI/2406.html;/home/schn/snap/zotero-snap/common/Zotero/storage/L3XECAGW/2406.html}
}

@book{mccartyHumanitiesComputing2005,
  title = {Humanities {{Computing}}},
  author = {McCarty, Willard},
  year = {2005},
  edition = {Paperback edition},
  publisher = {Palgrave Macmillan},
  address = {Basingstoke, Hampshire},
  isbn = {978-1-4039-3504-5 978-1-137-44042-6},
  lccn = {AZ105 .M245 2014},
  keywords = {commente,Digital humanities,Humanities,Information storage and retrieval systems,lu,Research Data processing},
  file = {/home/schn/snap/zotero-snap/common/Zotero/storage/YYXNRKV9/McCarty - 2014 - Humanities Computing.pdf}
}

@misc{monjourBarriereDernierKilometre2025,
  title = {{La barri{\`e}re du dernier kilom{\`e}tre}},
  author = {Monjour, Servanne and Sauret, Nicolas},
  year = {2025},
  month = may,
  address = {Toronto},
  langid = {french}
}

@inproceedings{napolesTheresNoComparison2016,
  title = {There's {{No Comparison}}: {{Reference-less Evaluation Metrics}} in {{Grammatical Error Correction}}},
  shorttitle = {There's {{No Comparison}}},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  author = {Napoles, Courtney and Sakaguchi, Keisuke and Tetreault, Joel},
  year = {2016},
  pages = {2109--2115},
  publisher = {Association for Computational Linguistics},
  address = {Austin, Texas},
  doi = {10.18653/v1/D16-1228},
  urldate = {2025-09-30},
  abstract = {Current methods for automatically evaluating grammatical error correction (GEC) systems rely on gold-standard references. However, these methods suffer from penalizing grammatical edits that are correct but not in the gold standard. We show that reference-less grammaticality metrics correlate very strongly with human judgments and are competitive with the leading reference-based evaluation metrics. By interpolating both methods, we achieve state-of-the-art correlation with human judgments. Finally, we show that GEC metrics are much more reliable when they are calculated at the sentence level instead of the corpus level. We have set up a CodaLab site for benchmarking GEC output using a common dataset and different evaluation metrics.},
  langid = {english},
  file = {/home/schn/snap/zotero-snap/common/Zotero/storage/B9C6T938/Napoles et al. - 2016 - There's No Comparison Reference-less Evaluation Metrics in Grammatical Error Correction.pdf}
}

@phdthesis{schumacherPosteditionTraductionAutomatique2023,
  title = {{La post-{\'e}dition de traduction automatique en contexte d'apprentissage}},
  author = {Schumacher, Perrine},
  year = {2023},
  month = aug,
  address = {Li{\`e}ge},
  langid = {french},
  school = {Universite de Liege}
}

@inproceedings{sizovAnalysingTranslationArtifacts2024,
  title = {Analysing {{Translation Artifacts}}: {{A Comparative Study}} of {{LLMs}}, {{NMTs}}, and {{Human Translations}}},
  shorttitle = {Analysing {{Translation Artifacts}}},
  booktitle = {Proceedings of the {{Ninth Conference}} on {{Machine Translation}}},
  author = {Sizov, Fedor and {Espa{\~n}a-Bonet}, Cristina and Van Genabith, Josef and Xie, Roy and Dutta Chowdhury, Koel},
  year = {2024},
  pages = {1183--1199},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.wmt-1.116},
  urldate = {2025-09-22},
  langid = {english},
  file = {/home/schn/snap/zotero-snap/common/Zotero/storage/5K7UFK88/Sizov et al. - 2024 - Analysing Translation Artifacts A Comparative Study of LLMs, NMTs, and Human Translations.pdf}
}

@article{strzeleckiMyLastKnowledge2025,
  title = {`{{As}} of My Last Knowledge Update': {{How}} Is Content Generated by {{{\textsc{ChatGPT}}}} Infiltrating Scientific Papers Published in Premier Journals?},
  shorttitle = {`{{As}} of My Last Knowledge Update'},
  author = {Strzelecki, Artur},
  year = {2025},
  month = jan,
  journal = {Learned Publishing},
  volume = {38},
  number = {1},
  pages = {e1650},
  issn = {0953-1513, 1741-4857},
  doi = {10.1002/leap.1650},
  urldate = {2025-09-21},
  abstract = {Abstract             The aim of this paper is to highlight the situation whereby content generated by the large language model ChatGPT is appearing in peer-reviewed papers in journals by recognized publishers. The paper demonstrates how to identify sections that indicate that a text fragment was generated, that is, entirely created, by ChatGPT. To prepare an illustrative compilation of papers that appear in journals indexed in the Web of Science and Scopus databases and possessing Impact Factor and CiteScore indicators, the SPAR4SLR method was used, which is mainly applied in systematic literature reviews. Three main findings are presented: in highly regarded premier journals, articles appear that bear the hallmarks of the content generated by AI large language models, whose use was not declared by the authors (1); many of these identified papers are already receiving citations from other scientific works, also placed in journals found in scientific databases (2); and, most of the identified papers belong to the disciplines of medicine and computer science, but there are also articles that belong to disciplines such as environmental science, engineering, sociology, education, economics and management (3). This paper aims to continue and add to the recently initiated discussion on the use of large language models like ChatGPT in the creation of scholarly works.},
  langid = {english}
}

@article{turingComputingMachineryIntelligence1950,
  title = {Computing {{Machinery}} and {{Intelligence}}},
  author = {Turing, A. M.},
  year = {1950},
  month = oct,
  journal = {Mind},
  volume = {LIX},
  number = {236},
  pages = {433--460},
  issn = {0026-4423},
  doi = {10.1093/mind/LIX.236.433},
  urldate = {2025-04-02},
  file = {/home/schn/snap/zotero-snap/common/Zotero/storage/G2DRA7SU/Turing - 1950 - Computing Machinery and Intelligence.pdf;/home/schn/snap/zotero-snap/common/Zotero/storage/LXFVJKSM/TURING - 1950 - I.â€”COMPUTING MACHINERY AND INTELLIGENCE.pdf;/home/schn/snap/zotero-snap/common/Zotero/storage/3YUZVLGI/986238.html;/home/schn/snap/zotero-snap/common/Zotero/storage/IJGLTCWL/986238.html}
}

@article{vicenteHumansInheritArtificial2023,
  title = {Humans Inherit Artificial Intelligence Biases},
  author = {Vicente, Luc{\'i}a and Matute, Helena},
  year = {2023},
  month = oct,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {15737},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-42384-8},
  urldate = {2025-09-25},
  abstract = {Abstract             Artificial intelligence recommendations are sometimes erroneous and biased. In our research, we hypothesized that people who perform a (simulated) medical diagnostic task assisted by a biased AI system will reproduce the model's bias in their own decisions, even when they move to a context without AI support. In three experiments, participants completed a medical-themed classification task with or without the help of a biased AI system. The biased recommendations by the AI influenced participants' decisions. Moreover, when those participants, assisted by the AI, moved on to perform the task without assistance, they made the same errors as the AI had made during the previous phase. Thus, participants' responses mimicked AI bias even when the AI was no longer making suggestions. These results provide evidence of human inheritance of AI bias.},
  langid = {english},
  file = {/home/schn/snap/zotero-snap/common/Zotero/storage/CBSJUCDL/Vicente et Matute - 2023 - Humans inherit artificial intelligence biases.pdf}
}

@misc{vitali-rosatiManifestePourEtudes2025a,
  title = {Manifeste Pour Des {{{\'E}tudes Critiques}} de l'{{Intelligence Artificielle}}},
  author = {{Vitali-Rosati}, Marcello},
  year = {2025},
  month = apr,
  journal = {Culture num{\'e}rique. Pour une philosophie du num{\'e}rique},
  urldate = {2025-07-15},
  abstract = {Blogue de Marcello Vitali-Rosati},
  langid = {english},
  file = {/home/schn/snap/zotero-snap/common/Zotero/storage/EQKI9NVY/manifeste-ecia.html}
}

@misc{wangComprehensiveSurveyGrammar2020,
  title = {A {{Comprehensive Survey}} of {{Grammar Error Correction}}},
  author = {Wang, Yu and Wang, Yuelin and Liu, Jie and Liu, Zhuo},
  year = {2020},
  month = may,
  number = {arXiv:2005.06600},
  eprint = {2005.06600},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.06600},
  urldate = {2025-09-10},
  abstract = {Grammar error correction (GEC) is an important application aspect of natural language processing techniques. The past decade has witnessed significant progress achieved in GEC for the sake of increasing popularity of machine learning and deep learning, especially in late 2010s when near human-level GEC systems are available. However, there is no prior work focusing on the whole recapitulation of the progress. We present the first survey in GEC for a comprehensive retrospect of the literature in this area. We first give the introduction of five public datasets, data annotation schema, two important shared tasks and four standard evaluation metrics. More importantly, we discuss four kinds of basic approaches, including statistical machine translation based approach, neural machine translation based approach, classification based approach and language model based approach, six commonly applied performance boosting techniques for GEC systems and two data augmentation methods. Since GEC is typically viewed as a sister task of machine translation, many GEC systems are based on neural machine translation (NMT) approaches, where the neural sequence-to-sequence model is applied. Similarly, some performance boosting techniques are adapted from machine translation and are successfully combined with GEC systems for enhancement on the final performance. Furthermore, we conduct an analysis in level of basic approaches, performance boosting techniques and integrated GEC systems based on their experiment results respectively for more clear patterns and conclusions. Finally, we discuss five prospective directions for future GEC researches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/schn/snap/zotero-snap/common/Zotero/storage/5DSB6USV/Wang et al. - 2020 - A Comprehensive Survey of Grammar Error Correction.pdf;/home/schn/snap/zotero-snap/common/Zotero/storage/9PJFD44K/2005.html;/home/schn/snap/zotero-snap/common/Zotero/storage/VFBJB6PA/2005.html}
}

@inproceedings{yoshimuraReferencelessSubMetricsOptimized2020,
  title = {{{SOME}}: {{Reference-less Sub-Metrics Optimized}} for {{Manual Evaluations}} of {{Grammatical Error Correction}}},
  shorttitle = {{{SOME}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Yoshimura, Ryoma and Kaneko, Masahiro and Kajiwara, Tomoyuki and Komachi, Mamoru},
  editor = {Scott, Donia and Bel, Nuria and Zong, Chengqing},
  year = {2020},
  month = dec,
  pages = {6516--6522},
  publisher = {International Committee on Computational Linguistics},
  address = {Barcelona, Spain (Online)},
  doi = {10.18653/v1/2020.coling-main.573},
  urldate = {2025-09-30},
  abstract = {We propose a reference-less metric trained on manual evaluations of system outputs for grammatical error correction (GEC). Previous studies have shown that reference-less metrics are promising; however, existing metrics are not optimized for manual evaluations of the system outputs because no dataset of the system output exists with manual evaluation. This study manually evaluates outputs of GEC systems to optimize the metrics. Experimental results show that the proposed metric improves correlation with the manual evaluation in both system- and sentence-level meta-evaluation. Our dataset and metric will be made publicly available.},
  file = {/home/schn/snap/zotero-snap/common/Zotero/storage/Z3QJHWLZ/Yoshimura et al. - 2020 - SOME Reference-less Sub-Metrics Optimized for Manual Evaluations of Grammatical Error Correction.pdf}
}
