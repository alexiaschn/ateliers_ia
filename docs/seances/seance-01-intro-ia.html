<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alexia Schneider alexia.schneider@umontreal.ca (UdeM), Marcello Vitali-Rosati marcello.vitali.rosati@umontreal.ca (UdeM)">
<meta name="dcterms.date" content="2025-09-11">

<title>Intro</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7504fb6c014d9d4fd213538c81d38504.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Programme</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../ressources.html"> 
<span class="menu-text">Ressources</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://alexiaschn.github.io/ateliers_ia/seances/intro.html#/title-slide"> 
<span class="menu-text">Séance 1</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://alexiaschn.github.io/ateliers_ia/seances/correction.html"> 
<span class="menu-text">Séance 2</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        
    <div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="intro.html"><i class="bi bi-file-slides"></i>RevealJS</a></li></ul></div></div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Intro</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Alexia Schneider <code>alexia.schneider@umontreal.ca</code> (UdeM), Marcello Vitali-Rosati <code>marcello.vitali.rosati@umontreal.ca</code> (UdeM) </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 11, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="plan" class="level2">
<h2 class="anchored" data-anchor-id="plan">Plan</h2>
<p>Théorie :</p>
<ol type="1">
<li>Présentation de la série d’atelier</li>
<li>Qu’est-ce que l’IA ?</li>
<li>Intérêt d’étudier l’IA pour les SHS</li>
<li>Retours historiques</li>
<li>Typologie des IA</li>
<li>Cas d’usage et modélisation experte (ELIZA)</li>
<li>Principe fondamentaux de l’apprentissage machine (modèles spécialisés)</li>
<li>Les LLMs : les modèles généralistes</li>
</ol>
<p>Pratique :</p>
<ol type="1">
<li>visualisation et manipulation d’une approche experte</li>
<li>visualisation du principe de vectorisation</li>
<li>Paramètres de chatbots (duck.ai)</li>
<li>Interprétabilité des chatbots (Neuronpédia)</li>
<li>Prompt Engineering (ChainForge)</li>
<li>Intégration de chatbot localement (Ollama)</li>
</ol>
</section>
<section id="présentation-et-objectif-des-ateliers" class="level2">
<h2 class="anchored" data-anchor-id="présentation-et-objectif-des-ateliers">Présentation et objectif des ateliers</h2>
<p>Format : 4 séances de 2heures, sans inscription, participation libre (à justifier pour le certificat des Humanités Numériques)</p>
<p>Théorie et pratique en alternance au cours des deux heures.</p>
<p>Objectifs de la série d’atelier :</p>
<ul>
<li>Comprendre les fondamentaux de l’IA et son histoire</li>
<li>Obtenir des notions critiques sur le fonctionnement profond des outils</li>
<li>Tester et s’approprier des outils d’IA</li>
<li>Maîtriser le vocabulaire de la discipline</li>
</ul>
<p>Objectifs de cet atelier :</p>
<ul>
<li>Comprendre les différentes formes d’IA</li>
<li>Comprendre les enjeux liés à l’utilisation des LLM</li>
<li>Tester différents paramètres d’IA générative.</li>
<li>Installer localement des modèles de langue.</li>
</ul>
</section>
<section id="certificat-canadien-en-humanités-numériques" class="level2">
<h2 class="anchored" data-anchor-id="certificat-canadien-en-humanités-numériques">Certificat canadien en Humanités Numériques</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figcaption>Certificat canadien en HN</figcaption>
<p><img src="../seances/img/ccdhn1.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><img src="../seances/img/ccdhn2.png" class="img-fluid" alt="Certificat canadien en HN"> <img src="../seances/img/ccdhn3.png" class="img-fluid" alt="Certificat canadien en HN"></p>
<p><a href="https://ccdhhn.ca/">Information sur le certificat</a></p>
</section>
<section id="quest-ce-que-lia" class="level2">
<h2 class="anchored" data-anchor-id="quest-ce-que-lia">Qu’est ce que l’IA ?</h2>
<ul>
<li><p>Tout et rien : exemples : chatbot, détection sur des imageries médicales, HTR, DeepBlue.</p></li>
<li><p>le dernier mot à la mode. Le ‘numérique’ des années 2020. <span class="citation" data-cites="vitali-rosatiManifestePourEtudes2025">(<a href="#ref-vitali-rosatiManifestePourEtudes2025" role="doc-biblioref">Vitali-Rosati 2025</a>)</span>.</p></li>
</ul>
<p>Définition pratique : “un programme informatique qui effectue une prédiction.”</p>
<p><del>À quoi sert d’étudier l’IA pour les chercheur.se.s en SHS</del></p>
</section>
<section id="lia-et-les-shs" class="level2">
<h2 class="anchored" data-anchor-id="lia-et-les-shs">L’IA et les SHS</h2>
<p>Que peuvent faire les SHS pour l’IA ?</p>
<ul>
<li>participer à la réflexion actuelle sur son utilisation :
<ul>
<li>positionnements de revues et de conférences (pose un cadre, parfois un précédent)</li>
</ul></li>
<li>proposer une théorie critique de l’IA décentrées de l’effet ‘benchmarking’ (i.e.&nbsp;comparaison des modèles ou des entreprises qui les mettent à disposition)</li>
<li>proposer une avis sur l’utilisation de ces outils qui soit propre à sa discipline (ex: distinguer des usages en fonction des besoins particuliers de son domaine).</li>
</ul>
</section>
<section id="exemples-de-prises-de-position" class="level2">
<h2 class="anchored" data-anchor-id="exemples-de-prises-de-position">Exemples de prises de position</h2>
<blockquote class="blockquote">
<p>Both SUP and JHUP have increasingly embraced, tested, and deployed some AI tools and policies. Barbara has been clear in her support of responsible uses of AI and the necessity of leveraging these early days to stake a claim within the quickly evolving landscape. Like SUP, JHUP is building and testing its own tools for marketing, accessibility, and analytics, efforts which place our presses in a position to potentially build services that might in the future even benefit other university presses. <span class="citation" data-cites="mulliken2025AUPressesWeekinResidence2025">(<a href="#ref-mulliken2025AUPressesWeekinResidence2025" role="doc-biblioref">Mulliken 2025</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>we offer recommendations for citing generative AI, defined as a tool that “can analyze or summarize content from a huge set of information, including web pages, books and other writing available on the internet, and use that data to create original new content” (Weed). <span class="citation" data-cites="HowCiteGenerative2023">(<a href="#ref-HowCiteGenerative2023" role="doc-biblioref"><span>“How Do <span>I</span> Cite Generative <span>AI</span> in <span>MLA</span> Style?”</span> 2023</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>The uncomfortable truth for researchers and publishers who oppose AI slowly taking over human review is that they might not be able to prevent it. Should a researcher use AI to write the first pass of peer review and not disclose it — in contravention of publisher guidelines — that might not be detectable, says Hosseini, who is also one of the editors of the journal Accountability in Research. And if AI reviews become widespread, that could change the practice of science, says Priem. “Every researcher can run their own bespoke review service over the preprint/dataset landscape, flagging/extracting only the science they care about (at any “quality” level) they want that day,” he wrote on X earlier this year. That could start to eat into the roles of journals, by taking away the certification that peer review mediated by journals provides, he says. <span class="citation" data-cites="naddafAITransformingPeer2025">(<a href="#ref-naddafAITransformingPeer2025" role="doc-biblioref">Naddaf 2025</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>Building critical AI literacies is a process of empowerment that enables students and citizens to exercise independent judgment about whether or if to use this very new and largely untested commercial technology. <span class="citation" data-cites="criticalai">(<a href="#ref-rutgersCriticalAILiteracies2024" role="doc-biblioref"><strong>rutgersCriticalAILiteracies2024?</strong></a>)</span></p>
</blockquote>
</section>
<section id="brève-histoire-de-lia-pt.-1" class="level2">
<h2 class="anchored" data-anchor-id="brève-histoire-de-lia-pt.-1">Brève histoire de l’IA (pt.&nbsp;1)</h2>
<p>(1940s : Science-fiction et roman d’Isaac Asimov <em>Runaround</em> en 1942.)</p>
<p><span class="citation" data-cites="turingComputingMachineryIntelligence1950">(<a href="#ref-turingComputingMachineryIntelligence1950" role="doc-biblioref">Turing 1950</a>)</span> : ‘can machines think?’</p>
<p>1956: ‘intelligence artificielle’, Minsky et McCarthy à la Dartmouth Summer Research Project on Artificial Intelligence (DSRPAI).</p>
<p>1966 : ELIZA <span class="citation" data-cites="weizenbaumELIZAComputerProgram1966">(<a href="#ref-weizenbaumELIZAComputerProgram1966" role="doc-biblioref">Weizenbaum 1966</a>)</span></p>
<p>1990-2000s : pic des systèmes experts et des arbres de décision. DeepBlue d’IBM <span class="citation" data-cites="campbellDeepBlue2002">(<a href="#ref-campbellDeepBlue2002" role="doc-biblioref">Campbell, Hoane, and Hsu 2002</a>)</span>.</p>
</section>
<section id="brève-histoire-de-lia-pt.-2" class="level2">
<h2 class="anchored" data-anchor-id="brève-histoire-de-lia-pt.-2">Brève histoire de l’IA (pt.&nbsp;2)</h2>
<p>2010s : pic des systèmes d’IA avec une modélisation distributionnelle du language (vecteur). Word2Vec <span class="citation" data-cites="mikolovEfficientEstimationWord2013">(<a href="#ref-mikolovEfficientEstimationWord2013" role="doc-biblioref">Mikolov et al. 2013</a>)</span>, GloVE <span class="citation" data-cites="penningtonGloVeGlobalVectors2014">(<a href="#ref-penningtonGloVeGlobalVectors2014" role="doc-biblioref">Pennington, Socher, and Manning 2014</a>)</span>. Parmi les avancées majeures de cette modélisation on compte le mécanisme d’attention <span class="citation" data-cites="vaswaniAttentionAllYou2017">(<a href="#ref-vaswaniAttentionAllYou2017" role="doc-biblioref">Vaswani et al. 2017</a>)</span> et l’encodage bidirectionnel BERT <span class="citation" data-cites="devlinBERTPretrainingDeep2019">(<a href="#ref-devlinBERTPretrainingDeep2019" role="doc-biblioref">Devlin et al. 2019</a>)</span> qui permettent des modèles très performants comme le GPT-3 d’OpenAI <span class="citation" data-cites="brownLanguageModelsAre2020">(<a href="#ref-brownLanguageModelsAre2020" role="doc-biblioref">Brown et al. 2020</a>)</span>.</p>
<p>Actuellement : tendance à l’hybridation de ces modèles : Neuro-Symbolic Integration, Semantic Web Machine Learning <span class="citation" data-cites="marcusNextDecadeAI2020">(<a href="#ref-marcusNextDecadeAI2020" role="doc-biblioref">Marcus 2020</a>)</span>, {{&lt; cite "kautzThirdAISummer2022"&gt;}}, <span class="citation" data-cites="russellArtificialIntelligenceModern2022">(<a href="#ref-russellArtificialIntelligenceModern2022" role="doc-biblioref">Russell and Norvig 2022</a>)</span></p>
</section>
<section id="typologie-de-lia" class="level2">
<h2 class="anchored" data-anchor-id="typologie-de-lia">Typologie de l’IA</h2>
<ul>
<li><p>Approche experte ou modèle symbolique : modélisation d’un programme à partir de <strong>règles</strong> précises. Les règles doivent être applicables à de nouvelles données pour faire une prédiction.</p></li>
<li><p>Approche inductive ou modèle d’apprentissage machine (<em>machine learning</em>) : modélisation d’un programme à partir d’un grand volume de données. Ce sont les <strong>motifs de répétitions</strong> qui permettent à la machine d’émettre une prédiction.</p></li>
</ul>
</section>
<section id="ce-quil-faut-retenir" class="level2">
<h2 class="anchored" data-anchor-id="ce-quil-faut-retenir">Ce qu’il faut retenir</h2>
<ul>
<li>L’IA réfère à plusieurs type de modélisations pour la prédiction : une approche déductive et une approche inductive.</li>
<li>On parle ‘des saisons’ de l’IA pour évoquer l’attrait public de la discipline et son avancée depuis les années 1950.</li>
<li>Par conséquent, certaines approches attirent l’attention à un moment donné, actuellement IA = chatbot voire ChatGPT.</li>
<li>L’IA réfère à des algorithmes qui permettent d’automatiser une prise de décision et pas seulement à des programmes de génération textuelle.</li>
</ul>
</section>
<section id="partons-dun-exemple" class="level2">
<h2 class="anchored" data-anchor-id="partons-dun-exemple">Partons d’un exemple</h2>
<p>Objectif : obtenir un programme capable de classer une phrase selon une thématique prédéfinie.</p>
<p>Exemple : Classification d’un texte soit en “parle de fruit” soit en “ne parle pas de fruit”.</p>
</section>
<section id="modéliser-une-approche-experte" class="level2">
<h2 class="anchored" data-anchor-id="modéliser-une-approche-experte">Modéliser une approche experte</h2>
<ul>
<li>faire appel à un expert : un humain pour déterminer les règles qui définissent ce qui est une phrase parlant de fruits.</li>
<li>exemple de règle possible : liste de mots comme ‘pomme, pommes, banane, poire etc.’ ordre des mots ou POS pour distinguer ‘orange’ couleur du fruit par exemple.</li>
</ul>
<p>Une approche qui sembler simpliste en apparence mais qui :</p>
<ul>
<li>peut s’avérer très complexe (ex: traduction)</li>
<li>est la base de systèmes très performants</li>
<li>entre dans une logique de <em>lazy computing</em> <span class="citation" data-cites="fujinagaVirtuesLazyMachines2025">(<a href="#ref-fujinagaVirtuesLazyMachines2025" role="doc-biblioref">Fujinaga 2025</a>)</span></li>
<li>révèle les tâches de bas niveau pour passer d’une chaîne de caractères à un ensemble de caractéristiques : tokenisation, POS-tagging.</li>
</ul>
<p><a href="https://demo-atelier.streamlit.app/">Programme de démo</a></p>
</section>
<section id="exemple-dun-programme-conversationnel-génération-textuelle-avec-une-approche-experte-eliza" class="level2">
<h2 class="anchored" data-anchor-id="exemple-dun-programme-conversationnel-génération-textuelle-avec-une-approche-experte-eliza">Exemple d’un programme conversationnel /génération textuelle avec une approche experte ELIZA</h2>
<p>Try it yourself : <a href="https://anthay.github.io/eliza.html">ELIZA</a></p>
<blockquote class="blockquote">
<p>Eliza is a pattern-matching automated psychiatrist. Given a set of rules in the form of input/output patterns, Eliza will attempt to recognize user input phrases and generate relevant psychobabble responses. Each rule is specified by an input pattern and a list of output patterns. A pattern is a sentence consisting of space-separated words and variables. <span class="citation" data-cites="connellyElizapy">(<a href="#ref-connellyElizapy" role="doc-biblioref">Connelly n.d.</a>)</span></p>
</blockquote>
<p>Exemple de <em>literate programming</em> <span class="citation" data-cites="knuthLiterateProgramming1984">(<a href="#ref-knuthLiterateProgramming1984" role="doc-biblioref">Knuth 1984</a>)</span> :</p>
<p><a href="https://dhconnelly.com/paip-python/docs/paip/eliza.html">Lire le code d’ELIZA</a></p>
</section>
<section id="approche-inductive-le-machine-learning-classique" class="level2">
<h2 class="anchored" data-anchor-id="approche-inductive-le-machine-learning-classique">Approche inductive : le machine learning classique</h2>
<section id="e-étape-modélisation-des-données" class="level3">
<h3 class="anchored" data-anchor-id="e-étape-modélisation-des-données">1e étape Modélisation des données</h3>
<ul>
<li><strong>Constitution d’un corpus</strong> : obtenir un ensemble important de documents</li>
<li><strong>Annotation</strong> : attribution d’une classe à chaque document par un humain/expert, <em>ground truth</em> ou vérité de terrain.</li>
<li><strong>Encodage vectoriel</strong> : Comptage des tokens dans l’ensemble du jeu de données et dans chaque phrase/document.</li>
<li>On obtient une représentation vectorielle = coordonnées dans un espace vectoriel à <em>n</em> dimensions.</li>
</ul>
</section>
<section id="e-étape-choix-de-lalgorithme-de-classification" class="level3">
<h3 class="anchored" data-anchor-id="e-étape-choix-de-lalgorithme-de-classification">2e étape Choix de l’algorithme de classification</h3>
<p>Différentes logiques permettent de distinguer les données entre elles. Quelques exemples d’apprentissage machine classique :</p>
<ul>
<li>K-Nearest Neighbor -&gt; le token apartient à la même classe que ses voisins (au nombre K)</li>
<li>Arbre de décision -&gt; on construit un arbre de questions fermées qui dessine le jeu de données.</li>
<li>Regression logistique -&gt; une ligne sépare l’espace vectoriel entre les deux classes</li>
</ul>
</section>
<section id="e-étape-entraînement-supervisé-apprentissage-spécialisé" class="level3">
<h3 class="anchored" data-anchor-id="e-étape-entraînement-supervisé-apprentissage-spécialisé">3e étape Entraînement supervisé : apprentissage spécialisé</h3>
<p><strong>Ajustement des poids</strong> (valeurs des vecteurs) à partir de données spécialisées</p>
<p><a href="https://demo-atelier.streamlit.app/">Programme de démo</a></p>
</section>
</section>
<section id="approche-inductive-généraliste-les-llms" class="level2">
<h2 class="anchored" data-anchor-id="approche-inductive-généraliste-les-llms">Approche inductive généraliste : les LLMs</h2>
<p>Exemple de LLMs : BERT, GPT-4, Mixtral, Gemini, Llama, Qwen, DeepSeek etc.</p>
<section id="foundational-models-pré-entrainement" class="level3">
<h3 class="anchored" data-anchor-id="foundational-models-pré-entrainement">Foundational models : Pré-entrainement</h3>
<p><strong>Constitution d’un corpus non annoté</strong></p>
<p><strong>Apprentissage auto-supervisé</strong> : le modèle apprend à prédire le mot suivant ou remplir un blanc dans une phrase.</p>
<p><strong>Encodage itératif</strong> : chaque mot/token est encodé en vecteur (embeddings) et le réseau ajuste ses poids en fonction du contexte.</p>
<p>Dès cette étape on obtient un modèle généraliste capable de faire des prédictions à partir d’une requête en langue naturelle.</p>
</section>
<section id="fine-tuning-affinage." class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-affinage.">Fine-tuning affinage.</h3>
<p>Spécialisation du modèle sur une tâche précise à partir d’un jeu de données annotées.</p>
</section>
<section id="alignement" class="level3">
<h3 class="anchored" data-anchor-id="alignement">Alignement</h3>
<p><strong>Instruction-tuning</strong> : entraînement supervisé sur des données “question → réponse”.</p>
<p><strong>Reinforcement Learning with Human Feedback</strong> : des annotateurs évaluent les sorties du modèle, et un apprentissage par renforcement ajuste les préférences du modèle.</p>
</section>
</section>
<section id="en-résumé" class="level2">
<h2 class="anchored" data-anchor-id="en-résumé">En résumé</h2>
<p>Modèle de langue = modélisation de la langue dans son ensemble + capacité de prédiction.</p>
<p>Les LLMs font de la prédiction de token :</p>
<ul>
<li>la génération de texte n’est pas la première ni la seule utilisation des LLMs.</li>
<li>soliciter un LLM pour générer un texte demande de recalculer le token le plus probable à chaque token -&gt; coût énergétique important.</li>
</ul>
</section>
<section id="llms-et-chatbot" class="level2">
<h2 class="anchored" data-anchor-id="llms-et-chatbot">LLMs et chatbot</h2>
<p>Parce que les LLMs sont lourds (plusieurs Gigas) et parce qu’il est coûteux en énergie d’effectuer les calculs qui permettent de déterminer le prochain token (plusieurs GPU), l’usage le plus courant des IA générative est via le site propriétaire qui va interroger le modèle sur un serveur distant. C’est la forme ChatGPT, Mistral.ai, etc.</p>
</section>
<section id="duck.ai" class="level2">
<h2 class="anchored" data-anchor-id="duck.ai">Duck.ai</h2>
<p><a href="https://duck.ai">duck.ai</a> permet de comparer des modèles en interfaces chat tout en conservant des données privées.</p>
</section>
<section id="circuit-tracing" class="level2">
<h2 class="anchored" data-anchor-id="circuit-tracing"><em>Circuit Tracing</em></h2>
<p>Interprétation du méchanisme par lequel un modèle effectue produit une prédiction à partir d’un prompt.</p>
<p><a href="https://www.neuronpedia.org/gemma-2-2b/graph?slug=gemma-basket&amp;pruningThreshold=0.6&amp;densityThreshold=1">Neuronpedia ‘circuit tracing’ demo</a> : Explication du processus interne d’un LLM pour la prédiction d’un token à partir d’un prompt.</p>
<p><span class="citation" data-cites="ameisenCircuitTracingRevealing2025">(<a href="#ref-ameisenCircuitTracingRevealing2025" role="doc-biblioref">Ameisen et al. 2025</a>)</span></p>
</section>
<section id="ollama" class="level2">
<h2 class="anchored" data-anchor-id="ollama">Ollama</h2>
<p>Il est possible de faire tourner un SLM (small language model) localement. Pour ce faire : <code>ollama</code> est une bibliothèque qui permet de télécharger et d’utiliser localement un LLMs.</p>
<section id="installation-et-utilisation-de-ollama" class="level3">
<h3 class="anchored" data-anchor-id="installation-et-utilisation-de-ollama">Installation et utilisation de Ollama</h3>
<p><a href="https://ollama.com/download">Téléchargement de Ollama</a></p>
</section>
<section id="utilisation-de-ollama-en-invite-de-commande" class="level3">
<h3 class="anchored" data-anchor-id="utilisation-de-ollama-en-invite-de-commande">Utilisation de Ollama en invite de commande</h3>
<p><code>ollama run llama3.2</code> -&gt; télécharge et lance le modèle.</p>
<p>““” -&gt; pour des instructions longues</p>
<p><code>/show info</code> -&gt; information sur le modèle téléchargé</p>
<p><code>ollama list</code> -&gt; liste des modèles téléchargés et utilisables</p>
<p><code>ollama rm llama3.2</code> -&gt; supprime un modèle</p>
</section>
</section>
<section id="paramètres-dun-modèle" class="level2">
<h2 class="anchored" data-anchor-id="paramètres-dun-modèle">Paramètres d’un modèle</h2>
<ul>
<li>Le <strong>seed</strong> (nombre que l’on peut choisir): les LLMs ont une variable aléatoire au moment de l’encodage des données et au moment du requêtage : le seed permet d’utiliser toujours le même ordre aléatoire, càd d’obtenir pour un même prompt toujours la même réponse. Enjeu de reproductibilité.</li>
<li>La <strong>température</strong> (valeur de 0 à 1): détermine le degré d’utilisation de la variable aléatoire. Une température élevée signifie que le modèle sera plus “créatif” car il donnera plus probablement un token qui a une probabilité absolue moindre dans son contexte.</li>
<li><strong>top_k</strong> (valeur de 0 à 100): variable qui réduit la probabilité de générer des tokens absurdes. Une valeur élevée donne des réponses plus variées et une valeur basse des réponses plus conservatrices. (Défaut 40)</li>
<li><strong>top_p</strong> (valeur de 0 à 1): Fonctionne avec le top_k. Une valeur haute donne un texte varié, une valeur basse, un texte conservateur. (Défaut: 0,9)</li>
</ul>
<p>Source : <a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md#parameter">Documentation Ollama</a></p>
</section>
<section id="model-steering-ou-system-message" class="level2">
<h2 class="anchored" data-anchor-id="model-steering-ou-system-message">Model Steering ou System message</h2>
<p>Reconduire un modèle consiste à lui fournir des ordres qui vont modifier son comportement pour toutes les interactions suivantes : cette instruction initiale est le “System message”.</p>
<p><a href="https://www.neuronpedia.org/gemma-2-9b-it/steer">Steer model interactively on Neuronpedia</a></p>
<p><a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md">Tutoriel</a></p>
<p>Créer un nouveau document ‘Modelfile’ sans extension.</p>
<p>Linux : <code>cat &gt; Modelfile</code> puis CTRL+C :</p>
<blockquote class="blockquote">
<p>FROM llama3.2 PARAMETER temperature 1 top_k 100 top_p 1 seed 17 SYSTEM “Tu es un chien”</p>
</blockquote>
<p>puis CTRL+SHIFT+D et CTRL+D</p>
<p>Windows cmd (Win+R): <code>echo 'FROM llama3.2 PARAMETER temperature 1 top_k 100 top_p 1 seed 17 SYSTEM "Tu es un chien"' &gt; Modelfile</code> (CTRL+SHIFT+D)</p>
<p>Ou c/c manuellement :</p>
<blockquote class="blockquote">
<p>FROM llama3.2 PARAMETER temperature 1 top_k 100 top_p 1 seed 17 SYSTEM “Tu es un chien”</p>
</blockquote>
<p><code>ollama create chien -f Modelfile</code></p>
<p><code>ollama run chien</code></p>
</section>
<section id="limites-des-interfaces-de-chat" class="level2">
<h2 class="anchored" data-anchor-id="limites-des-interfaces-de-chat">Limites des interfaces de chat</h2>
<p>‘hacker un LLM’ avec du <em>prompt injection</em> ou autres techniques de <em>jailbreaking</em>.</p>
<p><a href="https://incidentdatabase.ai/">Incitent database</a></p>
<blockquote class="blockquote">
<p>Hidden prompts reportedly were discovered in at least 17 academic preprints on arXiv that purportedly instructed AI tools to deliver only positive peer reviews. The lead authors are reportedly affiliated with 14 institutions in eight countries, including Waseda University, KAIST, Peking University, and the University of Washington. The alleged concealed instructions, some of which were reportedly embedded using white text or tiny fonts, were purportedly intended to influence any reviewers who rely on AI tools. (https://incidentdatabase.ai/cite/1135)</p>
</blockquote>
<p>Les hallucinations : <strong>il n’y a pas d’hallucinations</strong>, toutes les générations produites par un LLMs ont la même teneur de vérité du point de vue de l’outil : le modèle ne peut pas évaluer sa réponse à l’aune d’un référentiel extérieur.</p>
</section>
<section id="le-prompt-engineering" class="level2">
<h2 class="anchored" data-anchor-id="le-prompt-engineering">Le prompt engineering</h2>
<p>Rendre un prompt robuste et surtout permettre l’évaluation systématique d’une stratégie de prompt. Réintégrer une forme de modélisation de son problème pour optimiser un prompt : le template.</p>
<p><a href="https://chainforge.ai/play/">ChainForge</a> : outil de comparaison de prompt : comparaison de modèle, comparaison de template (un texte qui inclut des variables) visualisation côte à côte des sorties.</p>
</section>
<section id="études-critiques-de-lia" class="level2">
<h2 class="anchored" data-anchor-id="études-critiques-de-lia">Études critiques de l’IA</h2>
<p>Discipline émergente : <a href="https://read.dukeupress.edu/critical-ai/issue/3/1">Critical AI revue</a> lancée en 2023.</p>
<p>Pistes de réflexions :</p>
<ul>
<li>Uniformisation des pratiques et des modes de pensées : l’interface de chat est une façon de formaliser son problème, quid de la recherche de solution en interrogeant des moteurs de recherche, des bases de données ou des archives spécialisées ?</li>
<li>Derrière l’apparente accessibilité de l’interface de chat, est-ce qu’on ne risque pas de creuser l’écart de la littératie numérique ?</li>
<li>Est-ce que ces connaissances spécifiques, comme celles du code, qui impliquent des capacités de raisonnement alternatives, ne risquent pas de se retrouver suelement dans une forme d’élite intellectuelle ?</li>
<li>Comment peut-on définir une littéracie propre aux outils d’IA ?</li>
<li>Quelle posture adopter ? Faut-il interdire l’usage dans la recherche ou l’enseignement, obliger une déclaration d’utilisation/citation ou encore laisser faire selon les usages et opter pour une approche pédagogique ?</li>
</ul>
</section>
<section id="ce-quil-faut-retenir-1" class="level2">
<h2 class="anchored" data-anchor-id="ce-quil-faut-retenir-1">Ce qu’il faut retenir</h2>
<ul>
<li>L’IA est amalgamé aux LLMs et en particulier aux interfaces de chatbots mais cela recouvre en réalité des processus algorithmiques variés.</li>
<li>L’histoire de l’IA a montré qu’il y a des phases tant dans les approches valorisées que dans l’approbation de l’‘intelligence artificielle’ opposée à l’intelligence humaine.</li>
<li>un système expert (symbolique) peut être aussi complexe et ‘intelligent’ qu’un LLM.</li>
<li>Les systèmes d’IA n’ont pas de connaissance du réel et sont des modèles purement probabilistes.</li>
<li>Les ‘halllucinations’ ne sont pas des anomalies, ce sont des erreurs que l’on qualifie a postériori comme telles.</li>
<li>Les systèmes inductifs sont appropriés pour certaines tâches : classification, production de résumé. Leur point fort reste leur adaptabilité à de nouveaux contextes.</li>
<li>Les chatbots sont des interfaces qui permettent un échange homme-machine en langue naturelle : l’exploitation des capacités inductives d’un LLMs ne nécessite pas de passer par une telle interface. Ex : classification, processus expérimental plus adapté à une utilisation sans cette interface.</li>
</ul>
</section>
<section id="ressources-vues-pendant-latelier" class="level2">
<h2 class="anchored" data-anchor-id="ressources-vues-pendant-latelier">Ressources vues pendant l’atelier</h2>
<p><a href="https://demo-atelier.streamlit.app/">Démo IA symbolique/IA connexioniste pour les ateliers</a></p>
<p><a href="https://duck.ai">Duck.ai</a> : Comparaison de modèles sous forme de chatbot et paramétrage.</p>
<p><a href="https://chainforge.ai/play/">ChainForge</a> : comparaison de prompts</p>
<p><a href="https://ollama.com/download">Ollama</a> et <a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md">documentation</a> : Téléchargement de LLM localement. Possibilité de <em>steer</em> un modèle.</p>
<p><a href="https://www.neuronpedia.org/gemma-2-9b-it/steer">Neuronpedia ‘steer’ demo</a> : Comparaison d’un modèle qui a été ‘redirigé’ ou non.</p>
<p><a href="https://www.neuronpedia.org/gemma-2-2b/graph?slug=gemma-basket&amp;pruningThreshold=0.6&amp;densityThreshold=1">Neuronpedia ‘circuit tracing’ demo</a> : Explication du processus interne d’un LLM pour la prédiction d’un token à partir d’un prompt.</p>
<p><a href="https://incidentdatabase.ai/">Incident Database AI</a> : Résumé des incidents et controverses relevées dans la presse lié aux IA (en anglais).</p>
<p><a href="https://read.dukeupress.edu/critical-ai">Critical AI journal</a> : Revue</p>
<!-- ## Annexe : glossaire


**document** : ici une phrase

**classes** : ensemble thématique de la classification. Ex : "fruit" et "non fruit" pour la classification binaire de notre exemple. 

**jeu de données** : ensemble des documents 

**apprentissage supervisé** : méthode d'apprentissage machine à partir de classes connues.

**apprentissage non-supervisé** : méthode d'apprentissage machine sans connaître les classes à l'avance : a pour objectif de déterminer les caractéristiques discriminantes d'un jeu de données.

**vérité de terrain** ou _ground truth_ : annotation effectuée par un humain sur l'ensemble du jeu de données.  -->
</section>
<section id="bibliographie" class="level2">

<!-- 
## Bibliographie
 
Ameisen, AUTHORS Emmanuel, Jack Lindsey, Adam Pearce, Wes Gurnee, Nicholas L. Turner, Brian Chen, Craig Citro, et al. 2025. “Circuit Tracing: Revealing Computational Graphs in Language Models.” Transformer Circuits. https://transformer-circuits.pub/2025/attribution-graphs/methods.html.

Breit, Anna, Laura Waltersdorfer, Fajar J. Ekaputra, Marta Sabou, Andreas Ekelhart, Andreea Iana, Heiko Paulheim, et al. 2023. “Combining Machine Learning and Semantic Web: A Systematic Mapping Study.” ACM Computing Surveys 55 (14s): 313:1–41. https://doi.org/10.1145/3586163.

Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” arXiv. https://doi.org/10.48550/arXiv.2005.14165.

Campbell, Murray, A. Joseph Hoane, and Feng-hsiung Hsu. 2002. “Deep Blue.” Artificial Intelligence 134 (1): 57–83. https://doi.org/10.1016/S0004-3702(01)00129-1.
Connelly, Daniel. n.d. “Eliza.py.” Eliza Emulation Python. https://dhconnelly.com/paip-python/docs/paip/eliza.html. Accessed August 20, 2025.

Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), edited by Jill Burstein, Christy Doran, and Thamar Solorio, 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1423.

Fujinaga, Ichiro. 2025. “On the virtues of lazy machines.” {Keynote}. Montréal.

Haenlein, Michael, and Andreas Kaplan. 2019. “A Brief History of Artificial Intelligence: On the Past, Present, and Future of Artificial Intelligence.” California Management Review 61 (4): 5–14. https://doi.org/10.1177/0008125619864925.

“How Do I Cite Generative AI in MLA Style?” 2023. MLA Style Center.

Knuth, D. E. 1984. “Literate Programming.” The Computer Journal 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.

Marcus, Gary. 2020. “The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence.” arXiv. https://doi.org/10.48550/arXiv.2002.06177.

Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” arXiv. https://doi.org/10.48550/arXiv.1301.3781.

Mulliken, Jasmine. 2025. “2025 AUPresses Week-in-Residence Report.”

Naddaf, Miryam. 2025. “AI Is Transforming Peer Review — and Many Scientists Are Worried.” Nature 639 (8056): 852–54. https://doi.org/10.1038/d41586-025-00894-7.

Pennington, Jeffrey, Richard Socher, and Christopher Manning. 2014. “GloVe: Global Vectors for Word Representation.” In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), edited by Alessandro Moschitti, Bo Pang, and Walter Daelemans, 1532–43. Doha, Qatar: Association for Computational Linguistics. https://doi.org/10.3115/v1/D14-1162.

Turing, A. M. 1950. “Computing Machinery and Intelligence.” Mind LIX (236): 433–60. https://doi.org/10.1093/mind/LIX.236.433.

Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” arXiv. https://doi.org/10.48550/arXiv.1706.03762.

Vitali-Rosati, Marcello. 2025. “Manifeste Pour Des Études Critiques de l’Intelligence Artificielle.” Culture Numérique. Pour Une Philosophie Du Numérique.

Weizenbaum, Joseph. 1966. “ELIZA—a Computer Program for the Study of Natural Language Communication Between Man and Machine.” Communications of the ACM 9 (1): 36–45. https://doi.org/10.1145/365153.365168. -->
<!-- Baliser les diapositives avec les shortcodes `{{< psectioni >}}` pour ouvrir et `{{< psectiono >}}` pour fermer. -->
<!-- 

{{< psectioni >}}

{{< psectiono >}} -->



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">Bibliographie</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-ameisenCircuitTracingRevealing2025" class="csl-entry" role="listitem">
Ameisen, AUTHORS Emmanuel, Jack Lindsey, Adam Pearce, Wes Gurnee, Nicholas L. Turner, Brian Chen, Craig Citro, et al. 2025. <span>“Circuit <span>Tracing</span>: <span>Revealing Computational Graphs</span> in <span>Language Models</span>.”</span> <em>Transformer Circuits</em>. https://transformer-circuits.pub/2025/attribution-graphs/methods.html.
</div>
<div id="ref-brownLanguageModelsAre2020" class="csl-entry" role="listitem">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language <span>Models</span> Are <span>Few-Shot Learners</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2005.14165">https://doi.org/10.48550/arXiv.2005.14165</a>.
</div>
<div id="ref-campbellDeepBlue2002" class="csl-entry" role="listitem">
Campbell, Murray, A. Joseph Hoane, and Feng-hsiung Hsu. 2002. <span>“Deep <span>Blue</span>.”</span> <em>Artificial Intelligence</em> 134 (1): 57–83. <a href="https://doi.org/10.1016/S0004-3702(01)00129-1">https://doi.org/10.1016/S0004-3702(01)00129-1</a>.
</div>
<div id="ref-connellyElizapy" class="csl-entry" role="listitem">
Connelly, Daniel. n.d. <span>“Eliza.py.”</span> <em>Eliza Emulation Python</em>. https://dhconnelly.com/paip-python/docs/paip/eliza.html. Accessed August 20, 2025.
</div>
<div id="ref-devlinBERTPretrainingDeep2019" class="csl-entry" role="listitem">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. <span>“<span>BERT</span>: <span class="nocase">Pre-training</span> of <span>Deep Bidirectional Transformers</span> for <span>Language Understanding</span>.”</span> In <em>Proceedings of the 2019 <span>Conference</span> of the <span>North American Chapter</span> of the <span>Association</span> for <span>Computational Linguistics</span>: <span>Human Language Technologies</span>, <span>Volume</span> 1 (<span>Long</span> and <span>Short Papers</span>)</em>, edited by Jill Burstein, Christy Doran, and Thamar Solorio, 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N19-1423">https://doi.org/10.18653/v1/N19-1423</a>.
</div>
<div id="ref-fujinagaVirtuesLazyMachines2025" class="csl-entry" role="listitem">
Fujinaga, Ichiro. 2025. <span>“<span>On the virtues of lazy machines</span>.”</span> {Keynote}. Montr<span>é</span>al.
</div>
<div id="ref-HowCiteGenerative2023" class="csl-entry" role="listitem">
<span>“How Do <span>I</span> Cite Generative <span>AI</span> in <span>MLA</span> Style?”</span> 2023. <em>MLA Style Center</em>.
</div>
<div id="ref-knuthLiterateProgramming1984" class="csl-entry" role="listitem">
Knuth, D. E. 1984. <span>“Literate <span>Programming</span>.”</span> <em>The Computer Journal</em> 27 (2): 97–111. <a href="https://doi.org/10.1093/comjnl/27.2.97">https://doi.org/10.1093/comjnl/27.2.97</a>.
</div>
<div id="ref-marcusNextDecadeAI2020" class="csl-entry" role="listitem">
Marcus, Gary. 2020. <span>“The <span>Next Decade</span> in <span>AI</span>: <span>Four Steps Towards Robust Artificial Intelligence</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2002.06177">https://doi.org/10.48550/arXiv.2002.06177</a>.
</div>
<div id="ref-mikolovEfficientEstimationWord2013" class="csl-entry" role="listitem">
Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. <span>“Efficient <span>Estimation</span> of <span>Word Representations</span> in <span>Vector Space</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1301.3781">https://doi.org/10.48550/arXiv.1301.3781</a>.
</div>
<div id="ref-mulliken2025AUPressesWeekinResidence2025" class="csl-entry" role="listitem">
Mulliken, Jasmine. 2025. <span>“2025 <span class="nocase">AUPresses Week-in-Residence Report</span>.”</span>
</div>
<div id="ref-naddafAITransformingPeer2025" class="csl-entry" role="listitem">
Naddaf, Miryam. 2025. <span>“<span>AI</span> Is Transforming Peer Review — and Many Scientists Are Worried.”</span> <em>Nature</em> 639 (8056): 852–54. <a href="https://doi.org/10.1038/d41586-025-00894-7">https://doi.org/10.1038/d41586-025-00894-7</a>.
</div>
<div id="ref-penningtonGloVeGlobalVectors2014" class="csl-entry" role="listitem">
Pennington, Jeffrey, Richard Socher, and Christopher Manning. 2014. <span>“<span>GloVe</span>: <span>Global Vectors</span> for <span>Word Representation</span>.”</span> In <em>Proceedings of the 2014 <span>Conference</span> on <span>Empirical Methods</span> in <span>Natural Language Processing</span> (<span>EMNLP</span>)</em>, edited by Alessandro Moschitti, Bo Pang, and Walter Daelemans, 1532–43. Doha, Qatar: Association for Computational Linguistics. <a href="https://doi.org/10.3115/v1/D14-1162">https://doi.org/10.3115/v1/D14-1162</a>.
</div>
<div id="ref-russellArtificialIntelligenceModern2022" class="csl-entry" role="listitem">
Russell, Stuart J., and Peter Norvig. 2022. <em>Artificial Intelligence: A Modern Approach</em>. Fourth edition, global edition. Prentice <span>Hall</span> Series in Artificial Intelligence. Boston: Pearson.
</div>
<div id="ref-turingComputingMachineryIntelligence1950" class="csl-entry" role="listitem">
Turing, A. M. 1950. <span>“Computing <span>Machinery</span> and <span>Intelligence</span>.”</span> <em>Mind</em> LIX (236): 433–60. <a href="https://doi.org/10.1093/mind/LIX.236.433">https://doi.org/10.1093/mind/LIX.236.433</a>.
</div>
<div id="ref-vaswaniAttentionAllYou2017" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention <span>Is All You Need</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1706.03762">https://doi.org/10.48550/arXiv.1706.03762</a>.
</div>
<div id="ref-vitali-rosatiManifestePourEtudes2025" class="csl-entry" role="listitem">
Vitali-Rosati, Marcello. 2025. <span>“Manifeste Pour Des <span class="nocase"><span>É</span>tudes Critiques</span> de l’<span>Intelligence Artificielle</span>.”</span> <em>Culture Num<span>é</span>rique. Pour Une Philosophie Du Num<span>é</span>rique</em>.
</div>
<div id="ref-weizenbaumELIZAComputerProgram1966" class="csl-entry" role="listitem">
Weizenbaum, Joseph. 1966. <span>“<span>ELIZA</span>—a Computer Program for the Study of Natural Language Communication Between Man and Machine.”</span> <em>Communications of the ACM</em> 9 (1): 36–45. <a href="https://doi.org/10.1145/365153.365168">https://doi.org/10.1145/365153.365168</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>